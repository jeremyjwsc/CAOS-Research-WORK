aolin24.uab.es
read_CSV_data:
     36, Loop not vectorized/parallelized: potential early exits
     86, Memory copy idiom, loop replaced by call to __c_mcopy4
     94, Loop not vectorized/parallelized: contains call
PointsToCentroides:
    109, Generating present(New_Centr[:],Centroides[:],Labels[:],Punts[:])
         Generating copy(pR[:1])
         Generating present(Sep[:])
    111, Accelerator kernel generated
         Generating Tesla code
        112, #pragma acc loop gang /* blockIdx.x */
        117, #pragma acc loop vector(128) /* threadIdx.x */
    117, Loop is parallelizable
    121, Accelerator kernel generated
         Generating Tesla code
        122, #pragma acc loop gang /* blockIdx.x */
        136, #pragma acc loop vector(128) /* threadIdx.x */
        143, Generating implicit reduction(+:dist1)
        144, Generating implicit reduction(+:dist2)
        159, #pragma acc loop seq
        163, #pragma acc loop vector(128) /* threadIdx.x */
        176, Generating implicit reduction(+:dist1)
        177, Generating implicit reduction(+:dist2)
        178, Generating implicit reduction(+:dist3)
        179, Generating implicit reduction(+:dist4)
        200, #pragma acc loop vector(128) /* threadIdx.x */
        209, #pragma acc loop vector(128) /* threadIdx.x */
    136, Loop is parallelizable
    159, Loop carried scalar dependence for dist1 at line 182
         Scalar last value needed after loop for min_arg1 at line 197,196
         Loop carried scalar dependence for min_arg1 at line 182
         Loop carried scalar dependence for dist1 at line 183
         Loop carried scalar dependence for dist3 at line 185
         Scalar last value needed after loop for min_arg2 at line 211,206
         Scalar last value needed after loop for min_arg1 at line 202
         Scalar last value needed after loop for min_arg2 at line 205
         Loop carried scalar dependence for min_arg2 at line 185
         Loop carried scalar dependence for dist3 at line 186
         Loop carried scalar dependence for dist4 at line 179
         Loop carried scalar dependence for min_dist1 at line 182
         Loop carried scalar dependence for dist2 at line 188
         Loop carried scalar dependence for dist1 at line 176
         Loop carried scalar dependence for min_dist2 at line 186
         Loop carried scalar dependence for dist1 at line 189
         Loop carried scalar dependence for dist2 at line 177
         Loop carried scalar dependence for min_dist1 at line 183
         Loop carried scalar dependence for min_dist2 at line 185
         Loop carried scalar dependence for dist3 at line 178
         Loop carried scalar dependence for dist2 at line 189
         Loop carried scalar dependence for dist4 at line 192,191
    163, Loop is parallelizable
    200, Loop is parallelizable
    209, Loop is parallelizable
    215, Accelerator kernel generated
         Generating Tesla code
        216, #pragma acc loop gang /* blockIdx.x */
        220, #pragma acc loop vector(128) /* threadIdx.x */
    220, Loop is parallelizable
main:
    279, Loop not vectorized/parallelized: contains call
    285, read_CSV_data inlined, size=52 (inline) file kmeanACC.c (10)
          36, Loop not vectorized/parallelized: potential early exits
          86, Memory copy idiom, loop replaced by call to __c_mcopy4
          94, Loop not vectorized/parallelized: contains call
    288, Generating copyout(Labels[:N])
         Generating copy(Centroides[:C*D])
         Generating copyin(Punts[:N*D])
    289, Generating copyout(Sep[:C])
         Generating create(New_Centr[:C*D])
         Loop not vectorized/parallelized: potential early exits
    307, Loop not vectorized/parallelized: contains call
    326, Loop not vectorized/parallelized: contains call
    338, Loop not vectorized/parallelized: contains call
==22151== NVPROF is profiling process 22151, command: ./km3
==22151== Error: File already exists: /home/pfc/jwilliams/kms/GPU/PerformanceDataTest/traceA3.txt
==22151== Use "-f" to overwrite existing file
==22151== Profiling application: ./km3
==22151== Profiling result:
   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name
1.8e+10s  421.87us                    -               -         -         -         -  3.3994MB  7.8691GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8530ms                    -               -         -         -         -  16.000MB  8.4324GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.7927ms                    -               -         -         -         -  16.000MB  8.7159GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8263ms                    -               -         -         -         -  16.000MB  8.5557GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8192ms                    -               -         -         -         -  16.000MB  8.5888GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8204ms                    -               -         -         -         -  16.000MB  8.5835GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8458ms                    -               -         -         -         -  16.000MB  8.4650GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8499ms                    -               -         -         -         -  16.000MB  8.4463GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8444ms                    -               -         -         -         -  16.000MB  8.4718GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8737ms                    -               -         -         -         -  16.000MB  8.3392GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8686ms                    -               -         -         -         -  16.000MB  8.3621GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8786ms                    -               -         -         -         -  16.000MB  8.3172GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8893ms                    -               -         -         -         -  16.000MB  8.2703GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8897ms                    -               -         -         -         -  16.000MB  8.2684GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8835ms                    -               -         -         -         -  16.000MB  8.2959GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.8891ms                    -               -         -         -         -  16.000MB  8.2710GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s  1.3518ms                    -               -         -         -         -  16.000MB  11.558GB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s     864ns                    -               -         -         -         -      544B  600.46MB/s      Pinned      Device  GeForce GTX 108         1        14  [CUDA memcpy HtoD]
1.8e+10s     768ns                    -               -         -         -         -        4B  4.9671MB/s      Device           -  GeForce GTX 108         1        14  [CUDA memset]
1.8e+10s  2.0800us              (2 1 1)       (128 1 1)        10        0B        0B         -           -           -           -  GeForce GTX 108         1        14  PointsToCentroides_111_gpu [91]
1.8e+10s  24.836ms          (65535 1 1)       (128 1 1)        31      112B      512B         -           -           -           -  GeForce GTX 108         1        14  PointsToCentroides_121_gpu [93]
1.8e+10s  3.1360us              (2 1 1)       (128 1 1)        23        0B        0B         -           -           -           -  GeForce GTX 108         1        14  PointsToCentroides_215_gpu [95]
1.8e+10s  1.3120us                    -               -         -         -         -        4B  2.9075MB/s      Device      Pinned  GeForce GTX 108         1        14  [CUDA memcpy DtoH]
1.8e+10s     736ns                    -               -         -         -         -        8B  10.366MB/s      Device      Pinned  GeForce GTX 108         1        14  [CUDA memcpy DtoH]
1.8e+10s  315.08us                    -               -         -         -         -  3.8147MB  11.823GB/s      Device      Pinned  GeForce GTX 108         1        14  [CUDA memcpy DtoH]
1.8e+10s  1.0240us                    -               -         -         -         -      544B  506.64MB/s      Device      Pinned  GeForce GTX 108         1        14  [CUDA memcpy DtoH]

Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.
SSMem: Static shared memory allocated per CUDA block.
DSMem: Dynamic shared memory allocated per CUDA block.
SrcMemType: The type of source memory accessed by memory operation/copy
DstMemType: The type of destination memory accessed by memory operation/copy
======== Warning: Metric "l1_shared_utilization" cannot be found on device 0.
======== Warning: Metric "alu_fu_utilization" cannot be found on device 0.
======== Warning: Metric "l2_l1_read_transactions" cannot be found on device 0.
======== Warning: Metric "l2_l1_write_transactions" cannot be found on device 0.
======== Warning: Metric "nc_l2_read_transactions" cannot be found on device 0.
======== Warning: Metric "l2_l1_read_throughput" cannot be found on device 0.
======== Warning: Metric "l2_l1_write_throughput" cannot be found on device 0.
======== Warning: Metric "nc_l2_read_throughput" cannot be found on device 0.
======== Warning: Metric "atomic_throughput" cannot be found on device 0.
==22161== NVPROF is profiling process 22161, command: ./km3
==22161== Error: File already exists: /home/pfc/jwilliams/kms/GPU/PerformanceDataTest/metricsA3.out
==22161== Use "-f" to overwrite existing file
